# Analyses descriptives bivariées
Réaliser une analyse bivariée désigne le fait d'étudier la relation qui peut exister entre deux variables. Dans ce chapitre, nous allons voir les procédures graphiques et calculatoires qui permettent d'étudier et de quantifier le degré de relation pouvant exister entre deux variables dans les cas suivants : entre deux variables quantitatives, entre deux variables qualitatives, et entre une variable quantitative et une variable qualitative. Comme dans le chapitre précédent, l'objectif est ici d'explorer et de décrire les données et leurs relations à l'échelle d'un échantillon, sans pour autant chercher à déterminer l'incertitude qu'il peut exister dans les statistiques calculées en vue de les utiliser pour réaliser une inférence dans la population représentée.

## Relation entre deux variables quantitatives
### Etudier graphiquement la relation
Comme dans le cadre d'analyses univariées, une bonne pratique, lorsqu'on étudie une relation bivariée, est de faire un graphique. Avec des variables quantitatives, il s'agit de montrer les valeurs d'une variable en fonction des valeurs de l'autre variable, chose que permet un simple nuage de points. Plusieurs types de relations peuvent alors être rencontrés, ces relations pouvant potentiellement s'apparenter à autant de fonctions mathématiques que l'on connaît. Parmi les plus connues, on a par exemple les relations linéaires, les relations logarithmiques, ou encore les relations quadratiques, illustrées ci-dessous.

```{r relationships illustrations, message = FALSE, warning = FALSE, echo = FALSE, fig.align ="center"}

# Jeu de données pour la relation linéaire
set.seed(123)
data_lin <- 
  tibble(x = rnorm(n = 500, mean = 1, sd = 3)) %>%
  mutate(y = 2 * x + rnorm(n = 500, mean = 0, sd = 1))

# Jeu de données pour la relation logarithmique
set.seed(123)
data_log <- 
  tibble(x = rnorm(n = 500, mean = 1, sd = 3)) %>%
  mutate(y = log(x) + rnorm(n = 500, mean = 0, sd = 1))

# Jeu de données pour la relation quadratique
set.seed(123)
data_quad <- 
  tibble(x = rnorm(n = 500, mean = 1, sd = 6)) %>%
  mutate(y = x ^ 2 + rnorm(n = 500, mean = 2, sd = 15))

# Figure
g_lin <- 
  ggplot(data = data_lin, aes(x = x, y = y)) + 
  geom_point() +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank()) +
  xlab("X") +
  ylab("Y") +
  ggtitle("Linéaire")

g_log <- 
  ggplot(data = data_log, aes(x = x, y = y)) + 
  geom_point() +
  scale_x_continuous(limits = c(-1, 15)) +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank()) +
  xlab("X") +
  ylab("Y") +
  ggtitle("Logarithmique")

g_quad <- 
  ggplot(data = data_quad, aes(x = x, y = y)) + 
  geom_point() +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank()) +
  xlab("X") +
  ylab("Y") +
  ggtitle("Quadratique")

(g_lin | g_log | g_quad)

```

Dans R, pour obtenir un nuage de points à partir d'un jeu de données, il est possible d'utiliser la fonction `ggplot()` en l'associant à la fonction `geom_point()` du package `ggplot2`, comme dans l'exemple ci-dessous qui utilise le jeu de données `mtcars` (qui est intégré à R de base) et les variables `hp` (*gross horsepower*) et `mpg` (*miles/US gallon*). Dans cet exemple, on peut voir que la relation semble gobalement linéaire négative (voire curvilinéaire négative si l'on donne de l'importance au point isolé à droite du graphique).

```{r scatterplot, fig.align = "center"}
ggplot(data = mtcars, aes(x = hp, y = mpg)) + 
  geom_point()
```

### Etudier numériquement la relation

*Le coefficient de corrélation de Pearson* 

Lorsque la relation étudiée semble linéaire, l'étude numérique classique consiste à calculer le coefficient de corrélation de Pearson, noté $r$, dont la valeur vise à renseigner dans quelle mesure le nuage de points représentant le lien entre les deux variables étudiées suit une droite. Avant de se lancer dans le calcul du coefficient de corrélation de Pearson pour étudier la relation entre une variable $X$ et une variable $Y$, il peut donc être utile de compléter le nuage de points montré ci-dessus avec une droite d'équation de type $Y = aX + b$. Cette équation serait la meilleure modélisation possible de la relation linéaire entre $X$ et $Y$, de telle sorte que parmi l'infinité d'équations qui pourraient lier $X$ à $Y$, c'est cette équation qui au total donnerait la plus petite erreur lorsque l'on voudrait prédire $Y$ à partir de $X$. Si $X$ et $Y$ sont liées de manière linéaire, alors le nuage des points relatifs aux deux variables devrait s'étaler le long de cette droite. Pour obtenir cette droite en plus du nuage de points, il est possible d'utiliser la fonction `geom_smooth()` du package `ggplot2`.

```{r scatterplot and line, fig.align ="center"}
ggplot(data = mtcars, aes(x = hp, y = mpg)) + 
  geom_point() +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
```

Dans la fonction `geom_smooth()` qui a été utilisée dans l'exemple ci-dessus, on note que l'argument `formula` pourrait être considéré comme facultatif car il s'agit ici de la configuration par défaut de la fonction. En revanche, l'argument `method` doit être ici configuré avec `"lm"` (pour *linear model*) car ce n'est pas la méthode graphique configurée par défaut dans la fonction. Enfin, l'argument `se` permet de montrer ou non un intervalle de confiance autour de la droite de régression, ce qui n'a pas été activé ici (par défaut, l'argument `se` est configuré pour montrer cet intervalle de confiance). Dans l'exemple montré ci-dessus, la représentation graphique encourage fortement à penser que l'un des types de relations à envisager prioritairement dans l'étude des deux variables est la relation linéaire. Cette information rend pertinente l'utilisation du coefficient de corrélation de Pearson pour une étude numérique de la relation en question.

La valeur du coefficient de corrélation de Pearson peut aller de 1 (suggérant une relation linéaire positive parfaite) à -1 (suggérant une relation linéaire négative parfaite). Des valeurs proches de 0 suggèreraient une abscence de relation linéaire. La formule du coefficient de corrélation de Pearson ($r$) pour un échantillon est notée ci-dessous : 

$$r_{X,Y} =  {\frac{COV_{X,Y}}{s_{X} s_{Y}}} =  {\frac{\sum_{i=1}^{N} (X{i} - \overline{X}) (Y{i} - \overline{Y})}{N-1}} {\frac{1}{s_{X} s_{Y}}},$$

$COV$ désignant la covariance entre les variables $X$ et $Y$, $X{i}$ et $Y{i}$ les valeurs de $X$ et $Y$ pour une observation $i$, $\overline{X}$ et $\overline{Y}$ les moyennes des variables $X$ et $Y$, $N$ le nombre d'observations, et $s_{X}$ et $s_{Y}$ les écarts-types respectifs des variables $X$ et $Y$. Cette formule indique que le coefficient de corrélation de Pearson s'obtient en divisant la covariance des deux variables étudiées par le produit de leurs écarts-types respectifs. 

Le tableau ci-dessous montre les premières étapes du calcul de la covariance pour des couples de variables fictifs $(X1,Y1)$, $(X1,Y2)$, et $(X1,Y3)$. En particulier, la partie droite du tableau (de X1Y1 à X1Y3) montre le calcul du produit $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ pour les différents couples de variables et cela pour chaque ligne du jeu de données.

```{r table cov, echo = FALSE}
X1 <- c(0, 2, 4, 6, 8, 10, 12)
Y1 <- X1
Y2 <- c(0, 1, 15, 5, 11, 3, 12)
Y3 <- -X1

knitr::kable(
df <- data.frame(X1 = X1, Y1 = Y1, Y2 = Y2, Y3 = Y3) %>%
  mutate(X1Y1 = (X1 - mean(X1)) * (Y1 - mean(Y1)),
         X1Y2 = (X1 - mean(X1)) * (Y2 - mean(Y2)),
         X1Y3 = (X1 - mean(X1)) * (Y3 - mean(Y3)))
)
```

Ce que ce tableau peut permettre de rendre compte, c'est que plus les deux variables étudiées évolueront de manière consistante dans des sens identiques comme avec $X1$ et $Y1$, ou de manière consistante dans des sens opposés comme avec $X1$ et $Y3$, plus les produits $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ donneront respectivement des grands scores positifs ou des grands scores négatifs, et moins les scores $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ à additionner pour le calcul de la covariance s'annuleront. En effet, avec une relation relativement linéaire et positive les scores seront plus systématiquement positifs, et avec une relation relativement linéaire et négative les scores seront plus systématiquement négatifs. Toutefois, lorsqu'on aura des variables qui n'évolueront pas de manière consistante dans le même sens ou dans un sens opposé comme avec $X1$ et $Y2$, les scores positifs et négatifs liés aux calculs $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ auront tendance à s'annuler et donneront lieu à une somme des scores $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ diminuée, et donc à une covariance et au final à un coefficient de corrélation de Pearson tirés vers 0. Ces différents cas de figure et les calculs $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ correspondants sont illustrés sur la figure ci-dessous. Sur cette figure, chaque carré correspond au calcul $(X{i} - \overline{X}) (Y{i} - \overline{Y})$, le carré étant bleu lorsque le résultat du calcul est positif, et rouge lorsque le résultat est négatif. L'aire d'un carré illustre la grandeur du score issu du calcul. Sur les figures de gauche et de droite, on distingue une relation linéaire parfaite, ce qui maximise les scores à additionner pour le calcul de la covariance, dans le positif pour la figure de gauche et dans le négatif pour la figure de droite. Au milieu, on remarque que le manque de relation linéaire donne lieu à des carrés à la fois bleus et rouges, indiquant que les scores associés aux calculs $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ de la covariance s'annulent et diminuent ainsi la valeur finale de la covariance.

```{r graph cov, fig.align ="center", echo = FALSE, fig.width = 10, warning = FALSE}
df <- 
  df %>%
  mutate(signX1Y1 = ifelse(X1Y1 > 0, "pos", "neg"),
         signX1Y2 = ifelse(X1Y2 > 0, "pos", "neg"),
         signX1Y3 = ifelse(X1Y3 > 0, "pos", "neg"))

COV1 <- sum(df$X1Y1) / (length(df$X1Y1) - 1)
COV2 <- sum(df$X1Y2) / (length(df$X1Y2) - 1)
COV3 <- sum(df$X1Y3) / (length(df$X1Y3) - 1)

library(grid)

g1 <- ggplot(data = df) + 
  geom_rect(aes(xmin = X1, ymin = Y1, xmax = mean(X1), ymax = mean(Y1), fill = signX1Y1), alpha = 0.3) +
  geom_smooth(aes(X1, Y1), formula = y ~ x, method = "lm", se = FALSE, color = "black") +
  geom_point(aes(X1, Y1), color = "black", size = 3) + 
  geom_vline(aes(xintercept = mean(X1)), color = "black") + 
  geom_hline(aes(yintercept = mean(Y1)), color = "black") +
  scale_x_continuous(breaks = seq(0, 12, 2)) +
  scale_y_continuous(breaks = seq(0, 12, 2)) +
  scale_fill_manual(values = c("blue", "red"), breaks = c("pos", "neg"), labels = c("Tire la corrélation vers 1", "Tire la corrélation vers -1")) +
  xlab("X1") +
  ylab("Y1") +
  labs(fill = "", subtitle = bquote(paste("COV"["X1,Y1"]* " = ", .(round(COV1), digits = 1), "; SD"["X1"]*"SD"["Y1"]* " = ", .(round(sd(X1) * sd(Y1)), digits = 1)))) +
  ggtitle(bquote(paste("r"["X1,Y1"]* " = ", .(cor(X1, Y1), digits = 2)))) +
  geom_segment(aes(x = 2.8, y = 9.5, xend = 5.9, yend = 8.5), arrow = arrow(length = unit(0.5, "cm")), size = 0.8) +
  annotate("text", label = bquote(italic(bar(X)["1"])), x = 1.8, y = 9.7, size = 7) +
  geom_segment(aes(x = 9, y = 4, xend = 7, yend = 5.9), arrow = arrow(length = unit(0.5, "cm")), size = 0.8) +
  annotate("text", label = bquote(italic(bar(Y)["1"])), x = 9.9, y = 4, size = 7) +
  theme(axis.title = element_text(size = 15))


g2 <- ggplot(data = df) +
  geom_rect(aes(xmin = X1, ymin = Y2, xmax = mean(X1), ymax = mean(Y2), fill = signX1Y2), alpha = 0.3) +
  geom_smooth(aes(X1, Y2), formula = y ~ x, method = "lm", se = FALSE, color = "black") +
  geom_point(aes(X1, Y2), color = "black", size = 3) + 
  geom_vline(aes(xintercept = mean(X1)), color = "black") + 
  geom_hline(aes(yintercept = mean(Y2)), color = "black") +
  scale_x_continuous(breaks = seq(0, 12, 2)) +
  scale_y_continuous(breaks = seq(0, 15, 3)) +
  scale_fill_manual(values = c("blue", "red"), breaks = c("pos", "neg"), labels =  c("Tire la corrélation vers 1", "Tire la corrélation vers -1")) +
  xlab("X1") +
  ylab("Y2") +
  labs(fill = "", subtitle = bquote(paste("COV"["X1,Y2"]* " = ", .(round(COV2), digits = 1), "; SD"["X1"]*"SD"["Y2"]* " = ", .(round(sd(X1) * sd(Y2)), digits = 1)))) +
  ggtitle(bquote(paste("r"["X1,Y2"]* " = ", .(round(cor(X1, Y2), digits = 1))))) +
  theme(axis.title = element_text(size = 15))


g3 <- ggplot(data = df) +
  geom_rect(aes(xmin = X1, ymin = Y3, xmax = mean(X1), ymax = mean(Y3), fill = signX1Y3), alpha = 0.3) +
  geom_smooth(aes(X1, Y3), formula = y ~ x, method = "lm", se = FALSE, color = "black") +
  geom_point(aes(X1, Y3), color = "black", size = 3) + 
  geom_vline(aes(xintercept = mean(X1)), color = "black") + 
  geom_hline(aes(yintercept = mean(Y3)), color = "black") +
  scale_x_continuous(breaks = seq(0, 12, 2)) +
  scale_y_continuous(breaks = seq(-15, 0, 3)) +
  scale_fill_manual(values = c("blue", "red"), breaks = c("pos", "neg"), labels =  c("Tire la corrélation vers 1", "Tire la corrélation vers -1")) +
  xlab("X1") +
  ylab("Y3") +
  labs(fill = "", subtitle = bquote(paste("COV"["X1,Y3"]* " = ", .(round(COV3), digits = 1), "; SD"["X1"]*"SD"["Y3"]* " = ", .(round(sd(X1) * sd(Y3)), digits = 1)))) +
  ggtitle(bquote(paste("r"["X1,Y3"]* " = ", .(round(cor(X1, Y3), digits = 1))))) +
  theme(axis.title = element_text(size = 15))

((g1 | g2) + plot_layout(guides = "collect") & theme(legend.position = "bottom", legend.text = element_text(size = 12)) | (g3 + theme(legend.position = "none")))
```

Dans R, le coefficient de corrélation de Pearson peut être obtenu avec la fonction `cor()`. Dans l'exemple ci-dessous qui reprend les variables du jeu de données `mtcars` utilisées plus haut, on observe un coefficient négatif, relativement proche de -1, suggérant une relation relativement linéaire et négative entre les variables étudiées.

```{r cor function}
cor(x = mtcars$hp, y = mtcars$mpg, method = "pearson")
```

Toutefois, la fonction `cor.test()` sera plus intéressante pour la suite car elle permet de calculer des indices statistiques de probabilité qui seront nécessaires dès lors qu'il s'agira de chercher à inférer la valeur d'une corrélation dans une population d'où l'échantillon étudié provient. La valeur de la corrélation est  donnée à la fin de la liste des informations qui apparaissent suite à l'activation de la fonction.

```{r cor.test function}
cor.test(x = mtcars$hp, y = mtcars$mpg, method = "pearson")
```

Sur la base de travaux antérieurs, Hopkins et al. [-@Hopkins2009] ont fait une proposition de classification pour qualifier la valeur du coefficient de corrélation qui serait obtenue dans le cadre d'une relation linéaire. Cette proposition est montrée ci-dessous :

```{r correlation table, echo = FALSE}
knitr::kable(
  tribble(
    ~Petite, ~Moyenne, ~Grande, ~"Très grande", ~"Extrêmement grande",
    0.1,     0.3,      0.5,     0.7,            0.9
  )
)
```

Pour visualiser le lien que l'on peut faire entre la forme du nuage de points et la valeur du coefficient de corrélation de Pearson que l'on peut obtenir, la page web proposée par Kristoffer Magnusson (https://rpsychologist.com/correlation) peut être particulièrement intéressante. Cette page web donne la possibilité de faire varier manuellement la valeur du coefficient de corrélation de Pearson pour ensuite voir un nuage de points type correspondant à cette valeur. Faites un essai !

A noter que la valeur du coefficient de corrélation de Pearson est très dépendante de la manière dont sont distribuées les variables, avec une influence particulière de la variabilité des données [@Halperin1986]. L'influence de la variabilité est illustrée sur la figure ci-dessous. A gauche, on observe un nuage de points représentant une population complète, avec en conséquence une variablité le long des axes $X$ et $X$ relativement importante. La valeur du coefficient de corrélation de Pearson est ici particulièrement élevée. A droite, on observe exactement les mêmes valeurs, mais sur un intervalle dont l'étendue a été manuellement restreinte, diminuant ainsi la variabilité. On observe alors une diminution de la valeur du coefficient de corrélation de Pearson, alors qu'il s'agit à l'origine du même jeu de données que celui de gauche. Cet exemple doit faire prendre conscience qu'il faut faire attention lorsqu'on cherche à comparer des coefficient de corrélation de Pearson obtenus avec différents échantillons présentant des caractéristiques différentes, car si ces échantillons n'ont pas les mêmes niveaux de variabilité, les valeurs des coefficients de corrélation ne seront pas vraiment comparables, en sachant que c'est l'échantillon qui présente la plus grande variabilité qui aura mathématiquement plus de chances de présenter une valeur de coefficient de corrélation plus élevée.

```{r spurious correlations, echo = FALSE, warning=FALSE, message=FALSE,  fig.align ="center"}
g_lin <- 
  ggplot(data = data_lin, aes(x = x, y = y)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(-8, 10)) +
  scale_y_continuous(limits = c(-20, 25)) +
  xlab("X") +
  ylab("Y") +
  ggtitle(bquote(paste("r = ", .(round(cor(x = data_lin$x, y = data_lin$y), 2)))))

g_lin_sel <- 
  ggplot(data = filter(data_lin, x >-1 & x < 2.5), aes(x = x, y = y)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(-8, 10)) +
  scale_y_continuous(limits = c(-20, 25)) +
  xlab("X") +
  ylab("Y") +
  ggtitle(bquote(paste("r = ", .(round(cor(x = filter(data_lin, x >-1 & x < 2.5)$x, y = filter(data_lin, x >-1 & x < 2.5)$y), 2)))))

g_lin | g_lin_sel
```

Un exemple extrême de l'influence de la variabilité des données sur la valeur du coefficient de corrélation de Pearson est montré sur la figure ci-dessous. Les deux graphiques montrent exactement les mêmes données, à ceci près que sur le graphique de droite, on a remplacé en ordonnées une valeur du graphique de gauche pour lui donner la valeur de 10. L'influence de cette action sur la valeur du coefficient de corrélation est particulièrement nette, alors qu'une seule valeur a été modifée. Ceci montre qu'il faut faire attention aux valeurs extrêmes qui pourraient grandement influencer la valeur de corrélation obtenue, notamment en présence d'échantillons de taille relativement faible. Dans le cas où la valeur du coefficient de corrélation de Pearson serait très influencée par une valeur, il pourrait être une bonne pratique de calculer la valeur du coefficient de corrélation avec et sans cette valeur afin de pouvoir quantifier son influence sur la relation étudiée [@Halperin1986]. Une alternative pourrait être aussi d'étudier la relation à l'aide d'autres types de coefficients que celui de Pearson, tels que celui de Spearman, présenté plus bas. Toutes ces informations doivent en tous les cas faire prendre conscience qu'il n'est pas toujours pertinent de calculer le coefficient de corrélation de Pearson. En ce sens, lorsqu'on cherche à inférer la valeur du coefficient de corrélation de Pearson dans la population étudiée, et cela avec un degré d'incertitude bien défini, il convient de vérifier certains prérequis, lesquels seront abordés plus tard dans ce livre. 

```{r spurious correlation bis,  echo = FALSE, , fig.align ="center", message = FALSE, warning = FALSE}
data1 <- tibble(x = c(0, 1, 2, 3, 4, 5), y = c(2, 4, 3, 2, 3, 4))
data2 <- tibble(x = c(0, 1, 2, 3, 4, 5), y = c(2, 4, 3, 2, 3, 10))


g1 <- ggplot(data = data1, aes(x = x, y = y)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_x_continuous(limits = c(-5, 10)) +
  scale_y_continuous(limits = c(-5, 15)) +
  xlab("X") +
  ylab("Y") +
  ggtitle(bquote(paste("r = ", .(round(cor(x = data1$x, y = data1$y), 2)))))

g2 <- ggplot(data = data2, aes(x = x, y = y)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_x_continuous(limits =c(-5, 10)) +
  scale_y_continuous(limits = c(-5, 15)) +
  xlab("X") +
  ylab("Y") +
 ggtitle(bquote(paste("r = ", .(round(cor(x = data2$x, y = data2$y), 2)))))

g1 | g2
  
```

Lorsque la relation étudiée ne semble pas linéaire mais s'apparente assez clairement à d'autres fonctions mathématiques, telles que des relations logarithmiques ou polynomiales, il est possible de transformer une des variables, voire les deux, pour rendre la relation linéaire et à nouveau étudiable à l'aide du coefficient de corrélation de Pearson [@Halperin1986]. Toutefois, il est aussi possible de créer des modèles de régression non linéaires afin de regarder si ces modèles correspondent bien aux données. La détermination et la validation d'un modèle non linéaire qui correspondrait bien aux données confirmerait alors que la relation étudiée a une forme particulière et potentiellement prédictible. Les procédures pour explorer différents modèles de régression (linéaires et non linéaires) sont abordées au chapitre suivant. Enfin, une dernière alternative possible, pour étudier la relation entre deux variables quantitatives dont les distributions ne permettraient pas d'utiliser correctement le coefficient de corrélation de Pearson, serait l'utilisation de coefficients de corrélation basés sur les rangs, tels que le coefficient de corrélation de Spearman.

*Le coefficient de corrélation de Spearman*

Lorsque le coefficient de corrélation de Pearson ne permet pas de caractériser fiablement le degré de relation linéaire **entre les valeurs** de deux variables (par exemple en présence de valeurs aberrantes au sein d'un échantillon de petite taille), une alternative peut être d'étudier le degré de relation linéaire **entre les rangs** de ces deux variables. Le rang, c'est le classement (ou la position) d'une observation donnée en fonction de sa valeur. Dans une variable, les observations avec les valeurs les plus faibles seront associées aux rangs les plus bas alors que les observations avec les valeurs les plus élevées seront associées aux rangs les plus élevés. Une illustration de la notion de rang est proposée ci-dessous pour la variable `hp` du jeu de données `mtcars`. Dans le tableau ci-dessous, les lignes ont été ordonnées sur la base des rangs de la variable `hp`. On pourra remarquer que dans le tableau, nous avons ce qu'on appelle des ex-aequos, c'est-à-dire que plusieurs observations peuvent présenter les mêmes valeurs, et donc avoir le même rang.

```{r table ranks, echo = FALSE}
knitr::kable(
 mtcars %>%
   select(hp) %>%
   mutate(hp_rank = rank(hp)) %>%
   arrange(hp_rank)
)
```

Le fait d'étudier l'existence d'une relation linéaire entre les rangs et non plus entre les valeurs de deux variables permet de s'affranchir de l'influence possible de valeurs très extrêmes, dans l'une et/ou l'autre variable, sur le calcul final de la corrélation. Pour déterminer alors la valeur de la corrélation, une manière de procéder est d'appliquer la méthode de calcul du coefficient de corrélation de Pearson en utilisant non plus les valeurs des variables, mais les rangs correspondants. Cette methode, c'est celle du calcul du coefficient de corrélation de Spearman (*rho*). Si l'on suit *stricto sensu* cette définition, nous pourrions alors utiliser le code suivant pour avoir le coefficient de corrélation de Spearman :

```{r cor function spearman}
cor(x = rank(mtcars$hp), y = rank(mtcars$mpg), method = "pearson")
```

Toutefois, il existe une manière plus directe d'écrire les choses avec la fonction `cor`, qui contient un argument spécifiquement dédié au calcul du *rho* de Spearman : 

```{r cor function spearman bis}
cor(x = mtcars$hp, y = mtcars$mpg, method = "spearman")
```

La fonction `cor.test` permet aussi de calculer le coefficient de corrélation de Spearman en fournissant d'autres informations potentiellement intéressantes pour donner une idée de la significativité de l'estimation de *rho* pour la population étudiée.

```{r cor.test function spearman, warning=FALSE}
cor.test(x = mtcars$hp, y = mtcars$mpg, method = "spearman")
```

Si l'on veut produire une représentation graphique qui illustre la valeur de *rho* obtenue, il pourrait être davantage pertinent de non plus montrer un nuage de points à partir des valeurs des variables mises en lien, mais un nuage de points à partir de leurs rangs respectifs.

```{r graph cor spearman, fig.align="center", message=FALSE}
mtcars %>%
  mutate(hp_rank = rank(hp), mpg_rank = rank(mpg)) %>%
  ggplot(aes(x = hp_rank, y = mpg_rank)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

En matière d'interprétation, des valeurs de *rho* positives indiqueront que les deux variables mises en lien tendent à augmenter simultanément, on parlera alors de relation monotone positive. Dans le cas inverse, des valeurs négatives indiqueront que les deux variables mises en lien tendent à diminuer simultanément, on parlera alors de relation monotone négative. A noter cependant que de par son calcul, la valeur de *rho* ne permet pas de renseigner sur la forme de relation qu'il pourrait y avoir entre les valeurs des deux variables (e.g., linéaire ou curvilinéaire par exemple). Ceci est illustré sur la figure ci-dessous. La figure de gauche montre la relation entre les valeurs des variables $X$ et $Y$, qui est caractérisée par un coefficient de corrélation de Spearman (*rho*) de 1, indiquant donc que la relation est parfaitement monotone positive, sans préjuger de la forme particulière que pourrait présenter la relation. Pour mieux comprendre pourquoi cette valeur de *rho* est de 1, la figure de droite montre la relation entre les rangs de ces deux variables $X$ et $Y$. On voit en effet que la relation entre les rangs est effectivement parfaitement linéaire.

```{r ranks vs values spearman, echo = FALSE, fig.align = "center"}
a <- data.frame(x = seq(1, 50, 1)) %>%
  mutate(y = log(x)) %>%
  ggplot(aes(x = x, y = y)) +
  xlab("X") +
  ylab("Y") +
  geom_point() +
  ggtitle("Y vs. X")

b <- data.frame(x = seq(1, 50, 1)) %>%
  mutate(y = log(x), rang_X = rank(x), rang_Y = rank(y)) %>%
  ggplot(aes(x = rang_X, y = rang_Y)) + 
  geom_point() +
  xlab("Rang X") +
  ylab("Rang Y") +
    ggtitle("Rang Y vs. Rang X")

a | b
```


## Relation entre deux variables qualitatives
### Le khi2 de Pearson


## Relation entre une variable quantitative et une variable qualitative
### Le rapport de corrélation


