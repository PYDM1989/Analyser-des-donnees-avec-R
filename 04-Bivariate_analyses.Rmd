# Analyses descriptives bivariées
Réaliser une analyse bivariée désigne le fait d'étudier la relation qui peut exister entre deux variables. Dans ce chapitre, nous allons voir les procédures graphiques et calculatoires qui permettent d'étudier et de caractériser la relation pouvant exister entre deux variables dans les cas suivants : entre deux variables quantitatives, entre deux variables qualitatives, et entre une variable quantitative et une variable qualitative. Comme dans le chapitre précédent, l'objectif est ici d'explorer et de décrire les données et leurs relations à l'échelle d'un échantillon, sans pour autant chercher à déterminer l'incertitude qu'il peut exister dans les valeurs obtenues en vue d'utiliser ces valeurs pour caractériser la population représentée.

## Relation entre deux variables quantitatives
### Visualiser graphiquement les données 
Comme dans le cadre d'analyses univariées, une bonne pratique, lorsqu'on étudie une relation bivariée, est de faire un graphique. Avec des variables quantitatives, il s'agit de montrer les valeurs d'une variable en fonction des valeurs de l'autre variable, chose que permet un simple nuage de points. Plusieurs types de relations peuvent alors être rencontrés, ces relations pouvant potentiellement s'apparenter à autant de fonctions mathématiques que l'on connaît. Parmi les plus connues, on a par exemple les relations linéaires, les relations logarithmiques, ou encore les relations quadratiques, illustrées ci-dessous.

```{r relationships illustrations, message = FALSE, warning = FALSE, echo = FALSE, fig.align ="center"}

# Jeu de données pour la relation linéaire
set.seed(123)
data_lin <- 
  tibble(x = rnorm(n = 500, mean = 1, sd = 3)) %>%
  mutate(y = 2 * x + rnorm(n = 500, mean = 0, sd = 1))

# Jeu de données pour la relation logarithmique
set.seed(123)
data_log <- 
  tibble(x = rnorm(n = 500, mean = 1, sd = 3)) %>%
  mutate(y = log(x) + rnorm(n = 500, mean = 0, sd = 1))

# Jeu de données pour la relation quadratique
set.seed(123)
data_quad <- 
  tibble(x = rnorm(n = 500, mean = 1, sd = 6)) %>%
  mutate(y = x ^ 2 + rnorm(n = 500, mean = 2, sd = 15))

# Figure
g_lin <- 
  ggplot(data = data_lin, aes(x = x, y = y)) + 
  geom_point() +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank()) +
  ggtitle("Linéaire")

g_log <- 
  ggplot(data = data_log, aes(x = x, y = y)) + 
  geom_point() +
  scale_x_continuous(limits = c(-1, 15)) +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank()) +
  ggtitle("Logarithmique")

g_quad <- 
  ggplot(data = data_quad, aes(x = x, y = y)) + 
  geom_point() +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())  +
  ggtitle("Quadratique")

(g_lin | g_log | g_quad)

```

Dans R, pour obtenir un nuage de points à partir d'un jeu de données, il est possible d'utiliser la fonction `ggplot()` en l'associant à la fonction `geom_point()` du package `ggplot2`, comme dans l'exemple ci-dessous qui utilise le jeu de données `mtcars` (qui est intégré à R de base) et les variables `hp` (*gross horsepower*) et `mpg` (*miles/US gallon*). Dans cet exemple, on peut voir que la relation semble plutôt curvilinéaire négative (voire linéaire négative dans le cas où l'on ne considèrerait pas le point isolé à droite du graphique).

```{r scatterplot, fig.align = "center"}
ggplot(data = mtcars, aes(x = hp, y = mpg)) + 
  geom_point()
```

### Etudier l'existence d'une relation linéaire
*Réaliser un nuage de points avec droite de régression*

Avant de se lancer dans une étude numérique d'une relation linéaire entre une variable $X$ et une variable $Y$, il peut être utile de compléter le nuage de points montré ci-dessus avec une droite d'équation de type $Y = aX + b$ qui serait la meilleure modélisation possible de la relation linéaire entre $X$ et $Y$. Cette droite représente, parmi l'infinité d'équations qui pourraient lier $X$ à $Y$, celle qui au total donnerait la plus petite erreur lorsque l'on voudrait prédire $Y$ à partir de $X$. Si $X$ et $Y$ sont linéairement liées, le nuage des points relatifs aux deux variables devrait alors s'étaler le long de cette droite. Pour obtenir cette droite en plus du nuage de points, il est possible d'utiliser la fonction `geom_smooth()` du package `ggplot2`.

```{r scatterplot and line, fig.align ="center"}
ggplot(data = mtcars, aes(x = hp, y = mpg)) + 
  geom_point() +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
```

Dans la fonction `geom_smooth()` qui a été utilisée dans l'exemple ci-dessus, on note que l'argument `formula` pourrait être considéré comme facultatif car il s'agit ici de la configuration par défaut de la fonction. En revanche, l'argument `method` doit être ici configuré avec `"lm"` (pour *linear model*) car ce n'est pas la méthode graphique configurée par défaut dans la fonction. Enfin, l'argument `se` permet de montrer ou non un intervalle de confiance autour de la droite de régression, ce qui n'a pas été activé ici (par défaut, l'argument `se` est configuré pour montrer cet intervalle de confiance). Dans l'exemple montré ci-dessus, la représentation graphique encourage fortement à penser que l'un des types de relations à envisager prioritairement dans l'étude des deux variables est la relation linéaire. Cette information rend pertinente la conduite éventuelle d'une étude numérique d'une relation linéaire entre les deux variables.

*Calculer le coefficient de corrélation de Pearson (*$r$*)*

Classiquement, l'étude numérique d'une relation linéaire entre deux variables quantitatives s'effectue en calculant le coefficient de corrélation de Pearson, noté $r$. La valeur du $r$ de Pearson peut aller de 1 (suggérant une relation linéaire positive parfaite) à -1 (suggérant une relation linéaire négative parfaite). Des valeurs proches de 0 suggèreraient une abscence de relation linéaire. La formule du coefficient $r$ de Pearson pour un échantillon est notée ci-dessous : 

$$r_{X,Y} =  {\frac{COV_{X,Y}}{s_{X} s_{Y}}} =  {\frac{\sum_{i=1}^{N} (X{i} - \overline{X}) (Y{i} - \overline{Y})}{N-1}} {\frac{1}{s_{X} s_{Y}}},$$

$COV$ désignant la covariance entre les variables $X$ et $Y$, $X{i}$ et $Y{i}$ les valeurs de $X$ et $Y$ pour une observation $i$, $\overline{X}$ et $\overline{Y}$ les moyennes des variables $X$ et $Y$, $N$ le nombre d'observations, et $s_{X}$ et $s_{Y}$ les écarts-types respectifs des variables $X$ et $Y$. Cette formule indique que le coefficient de corrélation de Pearson s'obtient en divisant la covariance des deux variables étudiées par le produit de leurs écarts-types respectifs. 

Le tableau ci-dessous montre les premières étapes du calcul de la covariance pour des couples de variables fictifs $(X1,Y1)$, $(X1,Y2)$, et $(X1,Y3)$. En particulier, la partie droite du tableau (de X1Y1 à X1Y3) montre le calcul du produit $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ pour les différents couples de variables et cela pour chaque ligne du jeu de données.

```{r table cov, echo = FALSE}
X1 <- c(0, 2, 4, 6, 8, 10, 12)
Y1 <- X1
Y2 <- c(0, 1, 15, 5, 11, 3, 12)
Y3 <- -X1

knitr::kable(
df <- data.frame(X1 = X1, Y1 = Y1, Y2 = Y2, Y3 = Y3) %>%
  mutate(X1Y1 = (X1 - mean(X1)) * (Y1 - mean(Y1)),
         X1Y2 = (X1 - mean(X1)) * (Y2 - mean(Y2)),
         X1Y3 = (X1 - mean(X1)) * (Y3 - mean(Y3)))
)
```

Ce que ce tableau peut permettre de rendre compte, c'est que plus les deux variables étudiées évolueront de manière consistante dans des sens identiques comme avec $X1$ et $Y1$, ou de manière consistante dans des sens opposés comme avec $X1$ et $Y3$, plus les produits $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ donneront respectivement des grands scores positifs ou des grands scores négatifs, et moins les scores $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ à additionner pour le calcul de la covariance s'annuleront. En effet, avec une relation relativement linéaire et positive les scores seront plus systématiquement positifs, et avec une relation relativement linéaire et négative les scores seront plus systématiquement négatifs. Toutefois, lorsqu'on aura des variables qui n'évolueront pas de manière consistante dans le même sens ou dans un sens opposé comme avec $X1$ et $Y3$, les scores positifs et négatifs liés aux calculs $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ auront tendance à s'annuler et donneront lieu à une somme des scores $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ diminuée, et donc à une covariance et au final à un coefficient de corrélation de Pearson tirés vers 0. Ces différents cas de figure et les calculs $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ correspondants sont illustrés sur la figure ci-dessous. Sur cette figure, chaque carré correspond au calcul $(X{i} - \overline{X}) (Y{i} - \overline{Y})$, le carré étant bleu lorsque le résultat du calcul est positif, et rouge lorsque le résultat est négatif. L'aire d'un carré illustre la grandeur du score issu du calcul. Sur les figures de gauche et de droite, on distingue une relation linéaire parfaite, ce qui maximise les scores à additionner pour le calcul de la covariance, dans le positif pour la figure de gauche et dans le négatif pour la figure de droite. Au milieu, on remarque que le manque de relation linéaire donne lieu à des carrés à la fois bleus et rouges, indiquant que les scores associés aux calculs $(X{i} - \overline{X}) (Y{i} - \overline{Y})$ de la covariance s'annulent et diminuent ainsi la valeur finale de la covariance.

```{r graph cov, fig.align ="center", echo = FALSE, fig.width = 10, warning = FALSE}
df <- 
  df %>%
  mutate(signX1Y1 = ifelse(X1Y1 > 0, "pos", "neg"),
         signX1Y2 = ifelse(X1Y2 > 0, "pos", "neg"),
         signX1Y3 = ifelse(X1Y3 > 0, "pos", "neg"))

COV1 <- sum(df$X1Y1) / (length(df$X1Y1) - 1)
COV2 <- sum(df$X1Y2) / (length(df$X1Y2) - 1)
COV3 <- sum(df$X1Y3) / (length(df$X1Y3) - 1)

library(grid)

g1 <- ggplot(data = df) + 
  geom_rect(aes(xmin = X1, ymin = Y1, xmax = mean(X1), ymax = mean(Y1), fill = signX1Y1), alpha = 0.3) +
  geom_smooth(aes(X1, Y1), formula = y ~ x, method = "lm", se = FALSE, color = "black") +
  geom_point(aes(X1, Y1), color = "black", size = 3) + 
  geom_vline(aes(xintercept = mean(X1)), color = "black") + 
  geom_hline(aes(yintercept = mean(Y1)), color = "black") +
  scale_x_continuous(breaks = seq(0, 12, 2)) +
  scale_y_continuous(breaks = seq(0, 12, 2)) +
  scale_fill_manual(values = c("blue", "red"), breaks = c("pos", "neg"), labels = c("Tire la corrélation vers 1", "Tire la corrélation vers -1")) +
  xlab("X1") +
  ylab("Y1") +
  labs(fill = "", subtitle = bquote(paste("COV"["X1,Y1"]* " = ", .(round(COV1), digits = 1), "; SD"["X1"]*"SD"["Y1"]* " = ", .(round(sd(X1) * sd(Y1)), digits = 1)))) +
  ggtitle(bquote(paste("r"["X1,Y1"]* " = ", .(cor(X1, Y1), digits = 2)))) +
  geom_segment(aes(x = 2.8, y = 9.5, xend = 5.9, yend = 8.5), arrow = arrow(length = unit(0.5, "cm")), size = 0.8) +
  annotate("text", label = bquote(italic(bar(X)["1"])), x = 1.8, y = 9.7, size = 7) +
  geom_segment(aes(x = 9, y = 4, xend = 7, yend = 5.9), arrow = arrow(length = unit(0.5, "cm")), size = 0.8) +
  annotate("text", label = bquote(italic(bar(Y)["1"])), x = 9.9, y = 4, size = 7) +
  theme(axis.title = element_text(size = 15))


g2 <- ggplot(data = df) +
  geom_rect(aes(xmin = X1, ymin = Y2, xmax = mean(X1), ymax = mean(Y2), fill = signX1Y2), alpha = 0.3) +
  geom_smooth(aes(X1, Y2), formula = y ~ x, method = "lm", se = FALSE, color = "black") +
  geom_point(aes(X1, Y2), color = "black", size = 3) + 
  geom_vline(aes(xintercept = mean(X1)), color = "black") + 
  geom_hline(aes(yintercept = mean(Y2)), color = "black") +
  scale_x_continuous(breaks = seq(0, 12, 2)) +
  scale_y_continuous(breaks = seq(0, 15, 3)) +
  scale_fill_manual(values = c("blue", "red"), breaks = c("pos", "neg"), labels =  c("Tire la corrélation vers 1", "Tire la corrélation vers -1")) +
  xlab("X1") +
  ylab("Y2") +
  labs(fill = "", subtitle = bquote(paste("COV"["X1,Y2"]* " = ", .(round(COV2), digits = 1), "; SD"["X1"]*"SD"["Y2"]* " = ", .(round(sd(X1) * sd(Y2)), digits = 1)))) +
  ggtitle(bquote(paste("r"["X1,Y2"]* " = ", .(round(cor(X1, Y2), digits = 1))))) +
  theme(axis.title = element_text(size = 15))


g3 <- ggplot(data = df) +
  geom_rect(aes(xmin = X1, ymin = Y3, xmax = mean(X1), ymax = mean(Y3), fill = signX1Y3), alpha = 0.3) +
  geom_smooth(aes(X1, Y3), formula = y ~ x, method = "lm", se = FALSE, color = "black") +
  geom_point(aes(X1, Y3), color = "black", size = 3) + 
  geom_vline(aes(xintercept = mean(X1)), color = "black") + 
  geom_hline(aes(yintercept = mean(Y3)), color = "black") +
  scale_x_continuous(breaks = seq(0, 12, 2)) +
  scale_y_continuous(breaks = seq(-15, 0, 3)) +
  scale_fill_manual(values = c("blue", "red"), breaks = c("pos", "neg"), labels =  c("Tire la corrélation vers 1", "Tire la corrélation vers -1")) +
  xlab("X1") +
  ylab("Y3") +
  labs(fill = "", subtitle = bquote(paste("COV"["X1,Y3"]* " = ", .(round(COV3), digits = 1), "; SD"["X1"]*"SD"["Y3"]* " = ", .(round(sd(X1) * sd(Y3)), digits = 1)))) +
  ggtitle(bquote(paste("r"["X1,Y3"]* " = ", .(round(cor(X1, Y3), digits = 1))))) +
  theme(axis.title = element_text(size = 15))

((g1 | g2) + plot_layout(guides = "collect") & theme(legend.position = "bottom", legend.text = element_text(size = 12)) | (g3 + theme(legend.position = "none")))
```

Dans R, le coefficient de corrélation de Pearson peut être obtenu avec la fonction `cor()`. Dans l'exemple ci-dessous qui reprend les variables du jeu de données `mtcars` utilisées plus haut, on observe un coefficient négatif, relativement proche de -1, suggérant une relation relativement linéaire et négative entre les variables étudiées.

```{r cor function}
cor(x = mtcars$hp, y = mtcars$mpg, method = "pearson")
```

Toutefois, la fonction `cor.test()` sera plus intéressante pour la suite car elle permet de calculer des indices statistiques de probabilité qui seront nécessaires dès lors qu'il s'agira de chercher à inférer la valeur d'une corrélation dans une population d'où l'échantillon étudié provient. La valeur de la corrélation est  donnée à la fin de la liste des informations qui apparaissent suite à l'activation de la fonction.

```{r cor.test function}
cor.test(x = mtcars$hp, y = mtcars$mpg, method = "pearson")
```

Sur la base de travaux antérieurs, Hopkins et al. [-@Hopkins2009] ont fait une proposition de classification pour qualifier la valeur du coefficient de corrélation qui serait obtenue dans le cadre d'une relation linéaire. Cette proposition est montrée ci-dessous :

```{r table correlation, echo = FALSE}
knitr::kable(
  tribble(
    ~Petite, ~Moyenne, ~Grande, ~"Très grande", ~"Extrêmement grande",
    0.1,     0.3,      0.5,     0.7,            0.9
  )
)
```

*Calculer le coefficient de détermination (*$R^2$*) et l'erreur typique d'estimation (*$SEE$*) associés à un modèle linéaire*

Au-delà de la simple valeur du coefficient de corrélation de Pearson, il est possible d'investiguer l'existence d'une relation linéaire entre deux variables en modélisant de manière linéaire cette relation à l'aide d'une équation de type $Y = aX + b$, et en calculant certaines statistiques associées qui rendent compte du niveau de correspondance entre le modèle linéaire et les données étudiées. Ces statistiques sont le coefficient de détermination, noté $R^2$, et l'erreur typique d'estimation, dont on gardera l'acronyme anglais $SEE$ (pour *Standard Error of Estimate*).

Le coefficient $R^2$ représente la part de variance de la variable $Y$ expliquée par le modèle linéaire concerné. La formule de ce coefficient peut être présentée comme ceci : 

$$R^2 = 1 - {\frac {Var(\hat{Y} - Y) } {Var(Y)}} = 1 - {\frac {Var(RES)} {Var(Y)}}, $$
où $\hat{Y}$ désigne les prédictions faites à partir du modèle, et $Y$ désigne les valeurs réelles que l'on a cherché à prédire à partir du modèle. Le terme $\hat{Y} - Y$ (ou $RES$) doit se concevoir comme une variable contenant toutes les différences $\hat{Y}{i} - Y{i}$ qu'on appelle des **résidus**. Ainsi, le terme ${Var(\hat{Y} - Y) }$ désigne la variance des résidus (ou encore la variance des erreurs). Au final, le ratio ${\frac {Var(\hat{Y} - Y) } {Var(Y)}}$ traduit la part de variance non expliquée (non détectée) par le modèle, et le $R^2$ se calcule en faisant 1 moins ce ratio. (A noter qu'on peut trouver d'autres manières de présenter ce coefficient $R^2$, avec des formules initiales différentes, mais mathématiquement, ces différentes manières d'aborder les choses restent équivalentes).

La figure ci-dessous vise à représenter la notion de **résidu** et ce qu'il représente dans le calcul du $R^2$. Sur cette figure, les points représentent les valeurs $Y{i}$ en fonction des valeurs $X{i}$, la ligne bleue représente le modèle de régression linéaire (i.e., toutes les valeurs $\hat{Y}{i}$ qui seraient prédites à partir du modèle et des valeurs $X{i}$), et les segments rouges représentent les résidus (i.e., la différence qu'on a à chaque fois entre  $\hat{Y}{i}$ et $Y{i}$). Pour un modèle donné, plus ces segments rouges seront nombreux et grands, plus cela signifiera que les erreurs de prédiction du modèle sont nombreuses et grandes, que la part de variance non expliquée par le modèle est grande, et que la valeur du $R^2$ pour ce modèle est éloignée de 1. Ainsi, le coeffient $R^2$ peut aller de la valeur 0 (signifiant que le modèle n'explique aucune variation de $Y$), à la valeur de 1 (signifiant que le modèle explique toute les variations de $Y$). Plus la valeur de $R^2$ d'un modèle linéaire se rapprochera de 1, plus cela suggérera que la relation étudiée est effectivement linéaire.

```{r graph see, echo = FALSE, message = FALSE, fig.align = "center"}
a <- summary(lm(mpg ~ hp, data = mtcars))$coefficients[2]
b <- summary(lm(mpg ~ hp, data = mtcars))$coefficients[1]

dfR2 <- 
  mtcars %>%
  select(hp, mpg) %>%
  mutate(pred = a * hp + b,
         res = pred - mpg)

ggplot(data = dfR2) +
  geom_segment(aes(x = hp, xend = hp, y = pred, yend = mpg), color = "red", size = 1) +
  geom_point(aes(x = hp,y =  mpg), size = 3) +
  geom_smooth(aes(x = hp, y = mpg), method = "lm", se = FALSE) +
  xlab("X") +
  ylab("Y")
```

En réalité, le coefficient de corrélation de Pearson ($r$) et le coefficient de détermination ($R^2$) associé à un modèle linéaire sont mathématiquement liés, $r$ étant la racine carrée du $R^2$.

Pour déterminer le $R^2$ d'un modèle linéaire avec le logiciel R, il faut d'abord créer ce modèle à l'aide de la fonction `lm()`. L'usage simple de cette fonction, tel que montré  ci-dessous, permet de prendre connaissance des coefficients du modèle. Dans les résultats issus de l'exemple ci-dessous, l'ordonnée à l'origine est située sous `(Intercept)`, et le coefficient directeur est situé sous le nom de la variable $X$ du modèle, ici `hp`. Dans l'exemple ci-dessous, qui utilise à nouveau le jeu de données `mtcars`, le modèle nous indique que lorsque `hp` vaudra 0, l'estimation de `mpg` vaudra 30.09886, et que pour chaque augmentation d'unité de `hp`, on aura une diminution de -0.06823 unité de `mpg`.

```{r linear model}
lm(mpg ~ hp, data = mtcars)
```

Pour plus de confort dans l'écriture de la suite du code, il peut être intéressant d'associer le modèle crée avec la fonction `lm()` à un nom. Pour accéder aux différentes informations statistiques résumant le modèle, on peut alors utiliser la fonction `summary()` avec le nom choisi pour le modèle.

```{r linear model summary}
model <- lm(mpg ~ hp, data = mtcars)
summary(model)
```

Dans la liste d'informations données suite à l'activation du code, on retrouve notamment les coefficients déjà rencontrés plus haut, et on peut trouver le coefficient $R^2$ en face de l'écriture *Multiple R-squared*. On peut aussi y voir l'erreur typique d'estimation en face de l'écriture *Residual standard error*. 


L'erreur typique d'estimation, ou $SEE$, représente l'écart-type des erreurs d'estimation associées à l'utilisation d'un modèle. Son unité est donc celle de la variable $Y$ que l'on a cherché à prédire avec le modèle. La formule suivante permet d'expliquer son calcul à partir de données prélevées sur un échantillon :

$$ SEE = \sqrt{\frac{\sum_{i=1}^{N}(RES{i} - \overline{RES})^2}{N-2}}, $$

où $RES{i}$ désigne le résidu pour une observation donnée, $\overline{RES}$ la moyenne des résidus, et $N$ le nombre d'observations.

Il est possible d'extraire l'ordonnée à l'origine et la pente (i.e., le coefficient directeur) du modèle de régression, le coefficient $R^2$, et la statistique $SEE$, à partir de la liste d'informations obtenue avec la fonction `summary()`. Le code ci-dessous montre comment faire cela avec l'exemple concernant le jeu de données `mtcars` :

```{r coefficients extraction}
# Extraction de l'ordonnée à l'origine
intercept <- summary(model)$coefficients[1]
intercept

# Extraction du coefficient directeur
slope <-  summary(model)$coefficients[2]
slope

# Extraction du coefficient R2
R2 <- summary(model)$r.squared
R2

# Extraction de la statistique SEE
SEE <- summary(model)$sigma
SEE
```

Une fois extraites et associées à des noms, ces informations peuvent ensuite être réutilisées avec le package `ggplot2` et la fonction `annotate()` pour compléter le graphique initial avec des informations statistiques.

```{r graph lin and stats, fig.align = "center", message = FALSE, warning = FALSE}
ggplot(data = mtcars, aes(x = hp, y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  annotate("text", label = bquote(paste("Y = ", .(round(slope, digits = 3)), "X + ", .(round(intercept, digits = 3)))), x = 50, y = 35, hjust = 0, size = 5)
```

*La leçon du quartet d'Anscombe*

Dans cette partie dédiée à l'étude de la relation linéaire, il a été indiqué qu'il convient de réaliser au préalable un graphique montrant les données avant de s'adonner ensuite aux calculs visant la quantification d'une éventuelle relation linéaire. Cette première étape graphique est importante car les valeurs numériques qui peuvent être obtenues pour la corrélation de Pearson, le coefficient $R^2$, et la statistique $SEE$, ne peuvent à elles seules garantir l'aspect linéaire d'une relation. Un exemple qui permet d'illustrer cela est le quartet d'Anscombe [-@Anscombe1973]. Il s'agit de quatre jeux de données dont les représentations graphiques sont montrées ci-dessous.

```{r anscombe quartet data, echo = FALSE, results = FALSE}
anscombe <- 
  tribble(
  ~Ix,  ~Iy,	 ~IIx,  ~IIy,  ~IIIx,	~IIIy, ~IVx,  ~IVy,
  10.0,	8.04,	 10.0,  9.14,	 10.0,	7.46,	 8.0,	 6.58,
  8.0,	6.95,	 8.0,	  8.14,	 8.0,	  6.77,	 8.0,	 5.76,
  13.0,	7.58,	 13.0,  8.74,	 13.0,	12.74, 8.0,	 7.71,
  9.0,	8.81,	 9.0,	  8.77,	 9.0,	  7.11,	 8.0,	 8.84,
  11.0,	8.33,	 11.0,  9.26,	 11.0,	7.81,	 8.0,	 8.47,
  14.0,	9.96,	 14.0,  8.10,	 14.0,	8.84,	 8.0,	 7.04,
  6.0,	7.24,	 6.0,	  6.13,	 6.0,	  6.08,	 8.0,	 5.25,
  4.0,	4.26,	 4.0,	  3.10,	 4.0,	  5.39,	 19.0, 12.50,
  12.0,	10.84, 12.0,  9.13,	 12.0, 	8.15,  8.0,	 5.56,
  7.0,	4.82,	 7.0,	  7.26,	 7.0,	  6.42,	 8.0,	 7.91,
  5.0,	5.68,	 5.0,	  4.74,	 5.0,	  5.73,	 8.0,	 6.89
)
```

```{r anscombe quartet graph, echo = FALSE, fig.align = "center"}
size <- 3
color <- "red"
anscombe1 <- ggplot(data = anscombe, aes(x = Ix, y = Iy)) + 
  geom_point(size = size, color = color) + 
  geom_abline(slope = 0.5,  intercept = 3) +
  scale_x_continuous(breaks = seq(0, 20, 4), limits = c(0, 20)) +
  scale_y_continuous(breaks = seq(0, 15, 3), limits = c(0, 15)) +
  theme(axis.title = element_blank()) +
  annotate("text", x = 1.5, y = 14, label = "A", hjust = 1, size = 5)

anscombe2 <- ggplot(data = anscombe, aes(x = IIx, y = IIy)) + 
  geom_point(size = size, color = color) + 
  geom_abline(slope = 0.5,  intercept = 3) +
  scale_x_continuous(breaks = seq(0, 20, 4), limits = c(0, 20)) +
  scale_y_continuous(breaks = seq(0, 15, 3), limits = c(0, 15)) +
  theme(axis.title = element_blank()) +
  annotate("text", x = 1.5, y = 14, label = "B", hjust = 1, size = 5)

anscombe3 <- ggplot(data = anscombe, aes(x = IIIx, y = IIIy)) + 
  geom_point(size = size, color = color) + 
  geom_abline(slope = 0.5,  intercept = 3) + 
  scale_x_continuous(breaks = seq(0, 20, 4), limits = c(0, 20)) +
  scale_y_continuous(breaks = seq(0, 15, 3), limits = c(0, 15)) +
  theme(axis.title = element_blank()) +
  annotate("text", x = 1.5, y = 14, label = "C", hjust = 1, size = 5)

anscombe4 <- ggplot(data = anscombe, aes(x = IVx, y = IVy)) + 
  geom_point(size = size, color = color) + 
  geom_abline(slope = 0.5,  intercept = 3) + 
  scale_x_continuous(breaks = seq(0, 20, 4), limits = c(0, 20)) +
  scale_y_continuous(breaks = seq(0, 15, 3), limits = c(0, 15)) +
  theme(axis.title = element_blank()) +
  annotate("text", x = 1.5, y = 14, label = "D", hjust = 1, size = 5)

(anscombe1 | anscombe2) / (anscombe3 | anscombe4) 
```

Bien que d'aspects très différents, ces jeux de données montrent pourtant des variables en abscisses qui ont toutes la même moyenne ($\overline{X} = 9$) et le même écart-type ($SD = 3.32$), des variables en ordonnées qui ont elles aussi la même moyenne ($\overline{Y} = 7.5$) et le même écart-type = ($SD = 2.03$), et des lignes de régression linéaire qui ont toutes la même équation ($Y = 0.5X + 3$), le même coefficient de détermination ($R^2 = 0.67$) et la même erreur typique d'estimation ($SEE = 1.24$). Pour autant, on observe que seul le premier jeu de données (cf. graphique A ci-dessus) est associé à un modèle linéaire vraiment pertinent. En effet, le graphique B montre bien que la relation n'est pas linéaire mais plutôt quadratique, le graphique C montre que la régression est anormalement influencée par une valeur extrême, et le graphique D montre qu'il n'y a en réalité pas de relation linéaire entre les deux variables et que celle-ci ne semble exister numériquement que grâce à une seule valeur très extrême. Autant le graphique C invite à conserver une analyse de régression linéaire avec éventuellement certains ajustements pour la suite, autant les graphique B et D indiquent que la corrélation de Pearson et un modèle linéaire ne sont pas pertinents pour caractériser la relation entre les deux variables étudiées.

*Le coefficient de corrélation de Spearman*


*Le coefficient de corrélation de Kendall*










